"""
Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ø±Ø³Ø§Ù„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§
Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø§Ø®Ø¨Ø§Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡
"""

import json
import logging
from collections import Counter
from datetime import datetime
from typing import Dict, List
import os

from news_fetcher import get_collected_news

logger = logging.getLogger(__name__)

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ topics
TOPICS_FILE = "data/topics.json"
os.makedirs(os.path.dirname(TOPICS_FILE), exist_ok=True)


def load_topics() -> Dict:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ topics Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
    if os.path.exists(TOPICS_FILE):
        try:
            with open(TOPICS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading topics: {e}")
    return {}


def save_topics(topics: Dict):
    """Ø°Ø®ÛŒØ±Ù‡ topics"""
    try:
        with open(TOPICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Error saving topics: {e}")


def extract_keywords(title: str, min_word_length: int = 4) -> List[str]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±"""
    import re
    title_clean = re.sub(r'[^\w\s]', ' ', title.lower())
    stop_words = {
        'the', 'and', 'for', 'with', 'from', 'this', 'that', 'will', 
        'have', 'been', 'are', 'was', 'were', 'what', 'when', 'where',
        'who', 'why', 'how', 'about', 'after', 'before', 'into', 'through',
        'movie', 'film', 'new', 'first', 'more', 'gets', 'release', 'announced'
    }
    words = [w for w in title_clean.split() if len(w) >= min_word_length and w not in stop_words]
    return words


def calculate_similarity(title1: str, title2: str) -> float:
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ø¹Ù†ÙˆØ§Ù†"""
    keywords1 = set(extract_keywords(title1))
    keywords2 = set(extract_keywords(title2))
    if not keywords1 or not keywords2:
        return 0.0
    intersection = keywords1.intersection(keywords2)
    union = keywords1.union(keywords2)
    return len(intersection) / len(union) if union else 0.0


def group_similar_news(news_list: List[Dict], similarity_threshold: float = 0.4) -> List[List[Dict]]:
    """Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡"""
    groups = []
    used = set()
    for i, news1 in enumerate(news_list):
        if i in used:
            continue
        group = [news1]
        used.add(i)
        for j, news2 in enumerate(news_list[i+1:], start=i+1):
            if j in used:
                continue
            if calculate_similarity(news1['title'], news2['title']) >= similarity_threshold:
                group.append(news2)
                used.add(j)
        groups.append(group)
    return groups


def find_daily_trends(min_sources: int = 2) -> List[Dict]:
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø² Ø§Ø®Ø¨Ø§Ø± Ø°Ø®ÛŒØ±Ù‡â€ŒØ´Ø¯Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    today_news = get_collected_news()
    if not today_news:
        logger.info("No news found for today")
        return []
    groups = group_similar_news(today_news)
    trends = []
    for group in groups:
        if len(group) >= min_sources:
            sources = list(set([news.get('source', 'Unknown') for news in group]))
            best_title = max(group, key=lambda x: len(x.get('title', '')))['title']
            all_keywords = []
            for news in group:
                all_keywords.extend(extract_keywords(news['title']))
            keyword_counts = Counter(all_keywords)
            top_keywords = [kw for kw, _ in keyword_counts.most_common(3)]
            trend = {
                'title': best_title,
                'sources': sources,
                'source_count': len(sources),
                'news_count': len(group),
                'keywords': top_keywords,
                'urls': [news.get('url', '') for news in group[:5]],
                'timestamp': datetime.now().isoformat()
            }
            trends.append(trend)
    trends.sort(key=lambda x: x['source_count'], reverse=True)
    logger.info(f"Found {len(trends)} trends from {len(today_news)} news items")
    return trends


def format_trends_message(trends: List[Dict], max_trends: int = 10) -> str:
    """ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… ØªØ±Ù†Ø¯Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø²ÛŒØ¨Ø§"""
    if not trends:
        return "ğŸ” Ù‡ÛŒÚ† ØªØ±Ù†Ø¯ Ø®Ø¨Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯."
    trends = trends[:max_trends]
    today_date = datetime.now().strftime("%Y/%m/%d")
    message_parts = [
        "ğŸ“Š *ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*",
        f"ğŸ“… {today_date}",
        "",
        "ğŸ”¥ *Ø¯Ø§Øºâ€ŒØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ²:*",
        ""
    ]
    for idx, trend in enumerate(trends, 1):
        emoji = ["ğŸ¥‡","ğŸ¥ˆ","ğŸ¥‰"][idx-1] if idx <=3 else f"{idx}ï¸âƒ£"
        trend_text = [
            f"{emoji} *{trend['title']}*",
            f"   ğŸ“° Ù…Ù†Ø§Ø¨Ø¹: {', '.join(trend['sources'][:3])}",
        ]
        if len(trend['sources']) > 3:
            trend_text.append(f"   â• Ùˆ {len(trend['sources']) - 3} Ù…Ù†Ø¨Ø¹ Ø¯ÛŒÚ¯Ø±")
        trend_text.append(f"   ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {trend['news_count']}")
        if trend['urls'] and trend['urls'][0]:
            trend_text.append(f"   ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø±]({trend['urls'][0]})")
        trend_text.append("")
        message_parts.extend(trend_text)
    message_parts.extend([
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        "ğŸ¬ *Ø±Ø¨Ø§Øª Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*",
        f"â° Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {datetime.now().strftime('%H:%M')}"
    ])
    return "\n".join(message_parts)


def save_topic(title, link, source, date=None):
    """Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© Ø®Ø¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ±Ù†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    topics = load_topics()
    if not date:
        date = datetime.now().strftime("%Y-%m-%d")
    if date not in topics:
        topics[date] = []
    topic_data = {
        'title': title,
        'sources': [source],
        'source_count': 1,
        'news_count': 1,
        'keywords': extract_keywords(title),
        'urls': [link],
        'timestamp': datetime.now().isoformat()
    }
    topics[date].append(topic_data)
    save_topics(topics)
    logger.info(f"Saved topic: {title}")


# ØªØ³Øª
if __name__ == "__main__":
    trends = find_daily_trends()
    message = format_trends_message(trends)
    print(message)
