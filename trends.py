"""
Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ø±Ø³Ø§Ù„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§
"""
import json
import logging
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import os

logger = logging.getLogger(__name__)

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ topics
TOPICS_FILE = "data/topics.json"


def load_topics() -> Dict:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ topics Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
    if os.path.exists(TOPICS_FILE):
        try:
            with open(TOPICS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading topics: {e}")
    return {}


def save_topics(topics: Dict):
    """Ø°Ø®ÛŒØ±Ù‡ topics"""
    os.makedirs(os.path.dirname(TOPICS_FILE), exist_ok=True)
    try:
        with open(TOPICS_FILE, 'w', encoding='utf-8') as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Error saving topics: {e}")


# Backward compatibility: ØªØ§Ø¨Ø¹ Ù‚Ø¯ÛŒÙ…ÛŒ save_topic
def save_topic(topic_name: str, sources: List[str]):
    """
    ØªØ§Ø¨Ø¹ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
    Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© topic Ø¨Ø§ Ù„ÛŒØ³Øª Ù…Ù†Ø§Ø¨Ø¹
    """
    topics = load_topics()
    today_key = datetime.now().strftime("%Y-%m-%d")
    
    if today_key not in topics:
        topics[today_key] = []
    
    # Ø³Ø§Ø®Øª ÙØ±Ù…Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
    topic_data = {
        'title': topic_name,
        'sources': sources,
        'source_count': len(sources),
        'news_count': len(sources),
        'keywords': [],
        'urls': [],
        'timestamp': datetime.now().isoformat()
    }
    
    topics[today_key].append(topic_data)
    save_topics(topics)
    logger.info(f"Saved topic (legacy): {topic_name} with {len(sources)} sources")


def extract_keywords(title: str, min_word_length: int = 4) -> List[str]:
    """
    Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±
    
    Args:
        title: Ø¹Ù†ÙˆØ§Ù† Ø®Ø¨Ø±
        min_word_length: Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ†
    
    Returns:
        Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    """
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ
    import re
    title_clean = re.sub(r'[^\w\s]', ' ', title.lower())
    
    # Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬ Ú©Ù‡ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† keyword Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø¨Ø´Ù†
    stop_words = {
        'the', 'and', 'for', 'with', 'from', 'this', 'that', 'will', 
        'have', 'been', 'are', 'was', 'were', 'what', 'when', 'where',
        'who', 'why', 'how', 'about', 'after', 'before', 'into', 'through',
        'movie', 'film', 'new', 'first', 'more', 'gets', 'release', 'announced'
    }
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª
    words = [
        word for word in title_clean.split() 
        if len(word) >= min_word_length and word not in stop_words
    ]
    
    return words


def calculate_similarity(title1: str, title2: str) -> float:
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ø¹Ù†ÙˆØ§Ù†
    
    Returns:
        Ø¹Ø¯Ø¯ Ø¨ÛŒÙ† 0 ØªØ§ 1 Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù…ÛŒØ²Ø§Ù† Ø´Ø¨Ø§Ù‡Øª Ø§Ø³Øª
    """
    keywords1 = set(extract_keywords(title1))
    keywords2 = set(extract_keywords(title2))
    
    if not keywords1 or not keywords2:
        return 0.0
    
    # Jaccard similarity
    intersection = keywords1.intersection(keywords2)
    union = keywords1.union(keywords2)
    
    return len(intersection) / len(union) if union else 0.0


def group_similar_news(news_list: List[Dict], similarity_threshold: float = 0.4) -> List[List[Dict]]:
    """
    Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡
    
    Args:
        news_list: Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø±
        similarity_threshold: Ø­Ø¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª (0 ØªØ§ 1)
    
    Returns:
        Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ù…Ø´Ø§Ø¨Ù‡
    """
    groups = []
    used = set()
    
    for i, news1 in enumerate(news_list):
        if i in used:
            continue
        
        group = [news1]
        used.add(i)
        
        for j, news2 in enumerate(news_list[i+1:], start=i+1):
            if j in used:
                continue
            
            similarity = calculate_similarity(news1['title'], news2['title'])
            
            if similarity >= similarity_threshold:
                group.append(news2)
                used.add(j)
        
        groups.append(group)
    
    return groups


def find_daily_trends(news_list: List[Dict], min_sources: int = 3) -> List[Dict]:
    """
    Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡
    
    Args:
        news_list: Ù„ÛŒØ³Øª Ú©Ù„ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²
        min_sources: Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù† Ø¨Ù‡ ØªØ±Ù†Ø¯
    
    Returns:
        Ù„ÛŒØ³Øª ØªØ±Ù†Ø¯Ù‡Ø§ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ù…Ù„
    """
    # ÙÛŒÙ„ØªØ± Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ²
    today = datetime.now().date()
    today_news = [
        news for news in news_list 
        if datetime.fromisoformat(news.get('published', datetime.now().isoformat())).date() == today
    ]
    
    if not today_news:
        logger.info("No news found for today")
        return []
    
    # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡
    groups = group_similar_news(today_news)
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§ (Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ min_sources Ù…Ù†Ø¨Ø¹)
    trends = []
    
    for group in groups:
        if len(group) >= min_sources:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø§Ø¨Ø¹ ÛŒÚ©ØªØ§
            sources = list(set([news.get('source', 'Unknown') for news in group]))
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù†ÙˆØ§Ù† (Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† ÛŒØ§ Ø¬Ø§Ù…Ø¹â€ŒØªØ±ÛŒÙ†)
            best_title = max(group, key=lambda x: len(x.get('title', '')))['title']
            
            # Ø´Ù…Ø§Ø±Ø´ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
            all_keywords = []
            for news in group:
                all_keywords.extend(extract_keywords(news['title']))
            
            keyword_counts = Counter(all_keywords)
            top_keywords = [kw for kw, count in keyword_counts.most_common(3)]
            
            trend = {
                'title': best_title,
                'sources': sources,
                'source_count': len(sources),
                'news_count': len(group),
                'keywords': top_keywords,
                'urls': [news.get('url', '') for news in group[:5]],  # Ø­Ø¯Ø§Ú©Ø«Ø± 5 Ù„ÛŒÙ†Ú©
                'timestamp': datetime.now().isoformat()
            }
            
            trends.append(trend)
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ (Ø§Ø² Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ù‡ Ú©Ù…ØªØ±ÛŒÙ†)
    trends.sort(key=lambda x: x['source_count'], reverse=True)
    
    logger.info(f"Found {len(trends)} trends from {len(today_news)} news items")
    
    return trends


def format_trends_message(trends: List[Dict], max_trends: int = 10) -> str:
    """
    ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… ØªØ±Ù†Ø¯Ù‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù„ÛŒØ³Øª Ø²ÛŒØ¨Ø§
    
    Args:
        trends: Ù„ÛŒØ³Øª ØªØ±Ù†Ø¯Ù‡Ø§
        max_trends: Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ ØªØ±Ù†Ø¯ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´
    
    Returns:
        Ù¾ÛŒØ§Ù… ÙØ±Ù…Øª Ø´Ø¯Ù‡
    """
    if not trends:
        return "ğŸ” Ù‡ÛŒÚ† ØªØ±Ù†Ø¯ Ø®Ø¨Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯."
    
    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ ØªØ±Ù†Ø¯Ù‡Ø§
    trends = trends[:max_trends]
    
    # ØªØ§Ø±ÛŒØ® Ø§Ù…Ø±ÙˆØ²
    today_date = datetime.now().strftime("%Y/%m/%d")
    
    # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù…
    message_parts = [
        "ğŸ“Š *ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*",
        f"ğŸ“… {today_date}",
        "",
        "ğŸ”¥ *Ø¯Ø§Øºâ€ŒØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ²:*",
        ""
    ]
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‡Ø± ØªØ±Ù†Ø¯
    for idx, trend in enumerate(trends, 1):
        # Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ØªØ¨Ù‡
        if idx == 1:
            emoji = "ğŸ¥‡"
        elif idx == 2:
            emoji = "ğŸ¥ˆ"
        elif idx == 3:
            emoji = "ğŸ¥‰"
        else:
            emoji = f"{idx}ï¸âƒ£"
        
        # ÙØ±Ù…Øª ØªØ±Ù†Ø¯
        trend_text = [
            f"{emoji} *{trend['title']}*",
            f"   ğŸ“° Ù…Ù†Ø§Ø¨Ø¹: {', '.join(trend['sources'][:3])}",  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ù…Ù†Ø¨Ø¹
        ]
        
        # Ø§Ú¯Ø± Ø¨ÛŒØ´ Ø§Ø² 3 Ù…Ù†Ø¨Ø¹ Ø¯Ø§Ø±Ù‡
        if len(trend['sources']) > 3:
            trend_text.append(f"   â• Ùˆ {len(trend['sources']) - 3} Ù…Ù†Ø¨Ø¹ Ø¯ÛŒÚ¯Ø±")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±
        trend_text.append(f"   ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {trend['news_count']}")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ø§ÙˆÙ„ÛŒÙ† Ø®Ø¨Ø±
        if trend['urls'] and trend['urls'][0]:
            trend_text.append(f"   ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø±]({trend['urls'][0]})")
        
        trend_text.append("")  # Ø®Ø· Ø®Ø§Ù„ÛŒ Ø¨ÛŒÙ† ØªØ±Ù†Ø¯Ù‡Ø§
        
        message_parts.extend(trend_text)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† footer
    message_parts.extend([
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        "ğŸ¬ *Ø±Ø¨Ø§Øª Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*",
        f"â° Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {datetime.now().strftime('%H:%M')}"
    ])
    
    return "\n".join(message_parts)


# Backward compatibility: alias Ø¨Ø±Ø§ÛŒ ØªØ§Ø¨Ø¹ Ù‚Ø¯ÛŒÙ…ÛŒ
def format_trend_message(trends: List[Dict], max_trends: int = 10) -> str:
    """
    ØªØ§Ø¨Ø¹ Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
    """
    return format_trends_message(trends, max_trends)


def send_daily_trends(bot, chat_id: int, news_list: List[Dict], min_sources: int = 3):
    """
    Ø§Ø±Ø³Ø§Ù„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ù‡ Ú©Ø§Ù†Ø§Ù„
    
    Args:
        bot: Ù†Ù…ÙˆÙ†Ù‡ Ø±Ø¨Ø§Øª ØªÙ„Ú¯Ø±Ø§Ù…
        chat_id: Ø¢ÛŒØ¯ÛŒ Ú©Ø§Ù†Ø§Ù„/Ú¯Ø±ÙˆÙ‡ Ù…Ù‚ØµØ¯
        news_list: Ù„ÛŒØ³Øª Ú©Ù„ Ø§Ø®Ø¨Ø§Ø±
        min_sources: Ø­Ø¯Ø§Ù‚Ù„ ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù† Ø¨Ù‡ ØªØ±Ù†Ø¯
    """
    try:
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§
        trends = find_daily_trends(news_list, min_sources)
        
        if not trends:
            logger.info("No trends to send today")
            return
        
        # ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù…
        message = format_trends_message(trends)
        
        # Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù…
        bot.send_message(
            chat_id=chat_id,
            text=message,
            parse_mode='Markdown',
            disable_web_page_preview=False
        )
        
        logger.info(f"Successfully sent {len(trends)} trends to {chat_id}")
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØ±Ù†Ø¯Ù‡Ø§
        topics = load_topics()
        today_key = datetime.now().strftime("%Y-%m-%d")
        topics[today_key] = trends
        save_topics(topics)
        
    except Exception as e:
        logger.error(f"Error sending daily trends: {e}")


# ØªØ³Øª
if __name__ == "__main__":
    # Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ³Øª
    test_news = [
        {
            'title': 'Christopher Nolan Wins Best Director at Oscars 2024',
            'source': 'Variety',
            'published': datetime.now().isoformat(),
            'url': 'https://example.com/1'
        },
        {
            'title': 'Nolan Takes Home Best Director Oscar for Oppenheimer',
            'source': 'Hollywood Reporter',
            'published': datetime.now().isoformat(),
            'url': 'https://example.com/2'
        },
        {
            'title': 'Christopher Nolan Wins Oscar for Directing Oppenheimer',
            'source': 'Deadline',
            'published': datetime.now().isoformat(),
            'url': 'https://example.com/3'
        },
        {
            'title': 'Barbie Movie Breaks Box Office Records',
            'source': 'BoxOfficeMojo',
            'published': datetime.now().isoformat(),
            'url': 'https://example.com/4'
        },
    ]
    
    trends = find_daily_trends(test_news, min_sources=2)
    message = format_trends_message(trends)
    print(message)
