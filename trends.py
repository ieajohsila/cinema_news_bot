"""
Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø§Ø±Ø³Ø§Ù„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§
"""

import json
import logging
from collections import Counter
from datetime import datetime
import os

logger = logging.getLogger(__name__)

# ğŸ”§ FIX: Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ ØµØ­ÛŒØ­
DAILY_NEWS_DIR = "data/daily_news"
TOPICS_FILE = "data/topics.json"

# Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
os.makedirs(DAILY_NEWS_DIR, exist_ok=True)
os.makedirs(os.path.dirname(TOPICS_FILE), exist_ok=True)


def save_daily_news(news_item):
    """
    ğŸ”§ FIX: Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© Ø®Ø¨Ø± Ø¯Ø± ÙØ§ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡
    """
    today = datetime.now().strftime("%Y-%m-%d")
    today_file = os.path.join(DAILY_NEWS_DIR, f"{today}.json")
    
    try:
        # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ÙØ¹Ù„ÛŒ
        if os.path.exists(today_file):
            with open(today_file, "r", encoding="utf-8") as f:
                news_list = json.load(f)
        else:
            news_list = []
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯
        news_list.append({
            "title": news_item.get("title", ""),
            "url": news_item.get("link", news_item.get("url", "")),
            "source": news_item.get("source", "unknown"),
            "summary": news_item.get("summary", "")[:200],
            "timestamp": datetime.now().isoformat()
        })
        
        # Ø°Ø®ÛŒØ±Ù‡
        with open(today_file, "w", encoding="utf-8") as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        
        logger.debug(f"âœ… Ø®Ø¨Ø± Ø¯Ø± ÙØ§ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {today_file}")
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø± Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")


def load_topics():
    if os.path.exists(TOPICS_FILE):
        try:
            with open(TOPICS_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading topics: {e}")
    return {}


def save_topics(topics):
    try:
        with open(TOPICS_FILE, "w", encoding="utf-8") as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Error saving topics: {e}")


def save_topic(title, url, source):
    """Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯"""
    topics = load_topics()
    today = datetime.now().strftime("%Y-%m-%d")
    if today not in topics:
        topics[today] = []
    topics[today].append({
        "title": title,
        "sources": [source],
        "source_count": 1,
        "news_count": 1,
        "keywords": [],
        "urls": [url],
        "timestamp": datetime.now().isoformat()
    })
    save_topics(topics)


def extract_keywords(title, min_word_length=4):
    import re
    title_clean = re.sub(r'[^\w\s]', ' ', title.lower())
    stop_words = {'the', 'and', 'for', 'with', 'from', 'this', 'that', 'will',
                  'have', 'been', 'are', 'was', 'were', 'what', 'when', 'where',
                  'who', 'why', 'how', 'about', 'after', 'before', 'into', 'through',
                  'movie', 'film', 'new', 'first', 'more', 'gets', 'release', 'announced'}
    return [w for w in title_clean.split() if len(w) >= min_word_length and w not in stop_words]


def calculate_similarity(title1, title2):
    kws1 = set(extract_keywords(title1))
    kws2 = set(extract_keywords(title2))
    if not kws1 or not kws2:
        return 0.0
    return len(kws1 & kws2) / len(kws1 | kws2)


def group_similar_news(news_list, threshold=0.4):
    """Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡"""
    groups = []
    used = set()
    for i, n1 in enumerate(news_list):
        if i in used: 
            continue
        group = [n1]
        used.add(i)
        for j, n2 in enumerate(news_list[i+1:], start=i+1):
            if j in used: 
                continue
            if calculate_similarity(n1['title'], n2['title']) >= threshold:
                group.append(n2)
                used.add(j)
        groups.append(group)
    return groups


def find_daily_trends(min_sources=2):
    """
    ğŸ”§ FIX: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡
    """
    today = datetime.now().strftime("%Y-%m-%d")
    today_file = os.path.join(DAILY_NEWS_DIR, f"{today}.json")
    
    logger.info(f"ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ ØªØ±Ù†Ø¯Ù‡Ø§ Ø¯Ø±: {today_file}")
    
    if not os.path.exists(today_file):
        logger.warning(f"âš ï¸ ÙØ§ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ² ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯: {today_file}")
        return []
    
    try:
        with open(today_file, "r", encoding="utf-8") as f:
            news_list = json.load(f)
        
        logger.info(f"âœ… {len(news_list)} Ø®Ø¨Ø± Ø§Ø² ÙØ§ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯")
        
        if len(news_list) < min_sources:
            logger.info(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± ({len(news_list)}) Ú©Ù…ØªØ± Ø§Ø² Ø­Ø¯ minimum ({min_sources}) Ø§Ø³Øª")
            return []
        
        # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡
        groups = group_similar_news(news_list)
        logger.info(f"ğŸ“¦ {len(groups)} Ú¯Ø±ÙˆÙ‡ Ø®Ø¨Ø±ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")
        
        trends = []
        for group in groups:
            if len(group) >= min_sources:
                sources = list(set([n['source'] for n in group]))
                best_title = max(group, key=lambda x: len(x['title']))['title']
                
                all_keywords = []
                for n in group:
                    all_keywords.extend(extract_keywords(n['title']))
                top_keywords = [kw for kw, _ in Counter(all_keywords).most_common(3)]
                
                trends.append({
                    "title": best_title,
                    "sources": sources,
                    "source_count": len(sources),
                    "news_count": len(group),
                    "keywords": top_keywords,
                    "urls": [n['url'] for n in group[:5]],
                    "timestamp": datetime.now().isoformat()
                })
        
        trends.sort(key=lambda x: x['source_count'], reverse=True)
        logger.info(f"ğŸ”¥ {len(trends)} ØªØ±Ù†Ø¯ Ø¨Ø§ Ø­Ø¯Ø§Ù‚Ù„ {min_sources} Ù…Ù†Ø¨Ø¹ Ù¾ÛŒØ¯Ø§ Ø´Ø¯")
        
        return trends
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ±Ù†Ø¯Ù‡Ø§: {e}")
        return []


def format_trends_message(trends, max_trends=10):
    if not trends:
        return "ğŸ” Ù‡ÛŒÚ† ØªØ±Ù†Ø¯ Ø®Ø¨Ø±ÛŒ Ø§Ù…Ø±ÙˆØ² Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø¯."
    
    trends = trends[:max_trends]
    today_date = datetime.now().strftime("%Y/%m/%d")
    parts = ["ğŸ“Š *ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*", f"ğŸ“… {today_date}", "", "ğŸ”¥ *Ø¯Ø§Øºâ€ŒØªØ±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ²:*", ""]
    
    for idx, t in enumerate(trends, 1):
        emoji = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"{idx}ï¸âƒ£"
        txt = [
            f"{emoji} *{t['title']}*",
            f"   ğŸ“° Ù…Ù†Ø§Ø¨Ø¹: {', '.join(t['sources'][:3])}"
        ]
        if len(t['sources']) > 3:
            txt.append(f"   â• Ùˆ {len(t['sources'])-3} Ù…Ù†Ø¨Ø¹ Ø¯ÛŒÚ¯Ø±")
        txt.append(f"   ğŸ”¢ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {t['news_count']}")
        if t['urls']:
            txt.append(f"   ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø±]({t['urls'][0]})")
        txt.append("")
        parts.extend(txt)
    
    parts.extend(["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”", "ğŸ¬ *Ø±Ø¨Ø§Øª Ø®Ø¨Ø±ÛŒ Ø³ÛŒÙ†Ù…Ø§*", f"â° Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {datetime.now().strftime('%H:%M')}"])
    return "\n".join(parts)
