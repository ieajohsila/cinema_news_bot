"""
Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scraping
Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡
"""
"""news_fetcher.py
Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø³ÛŒÙ†Ù…Ø§ Ø§Ø² RSS Ùˆ Scrape
"""
import logging
from typing import List, Dict
import feedparser
import httpx
from bs4 import BeautifulSoup

from default_sources import DEFAULT_RSS_SOURCES, DEFAULT_SCRAPE_SITES

logger = logging.getLogger(__name__)


def fetch_rss(url: str) -> List[Dict]:
    """
    Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² RSS
    """
    articles: List[Dict] = []
    try:
        feed = feedparser.parse(url)
        entries = getattr(feed, 'entries', [])
        if not isinstance(entries, list):
            logger.error(f"âŒ RSS {url} Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª ÛŒØ§ entries Ù„ÛŒØ³Øª Ù†ÛŒØ³Øª")
            return []

        for entry in entries:
            article = {
                'title': entry.get('title', '').strip(),
                'url': entry.get('link', '').strip(),
                'published': entry.get('published', ''),
                'source': url
            }
            articles.append(article)

        logger.info(f"âœ… RSS: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url}: {e}")
    return articles


def fetch_scrape(url: str) -> List[Dict]:
    """
    Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Scrape
    """
    articles: List[Dict] = []
    try:
        response = httpx.get(url, timeout=15.0)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ùˆ Ø¹Ù†ÙˆØ§Ù† Ø§Ø®Ø¨Ø§Ø± (Ø§Ù„Ú¯ÙˆÛŒ Ø³Ø§Ø¯Ù‡)
        for a in soup.find_all('a', href=True):
            title = a.get_text(strip=True)
            link = a['href']

            # ÙÛŒÙ„ØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ ÛŒØ§ ØªØ¨Ù„ÛŒØº
            if not title or not link or link.startswith('#'):
                continue

            # Ú©Ø§Ù…Ù„ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù†Ø³Ø¨ÛŒ
            if link.startswith('/'):
                base = url.rstrip('/')
                link = f"{base}{link}"

            articles.append({
                'title': title,
                'url': link,
                'published': '',
                'source': url
            })

        logger.info(f"âœ… Scrape: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e}")
    return articles


def fetch_all_news() -> List[Dict]:
    """
    Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scrape
    """
    all_articles: List[Dict] = []

    logger.info(f"ðŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS ({len(DEFAULT_RSS_SOURCES)})...")
    for url in DEFAULT_RSS_SOURCES:
        articles = fetch_rss(url)
        if not isinstance(articles, list):
            articles = []
        all_articles.extend(articles)

    logger.info(f"ðŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Scrape ({len(DEFAULT_SCRAPE_SITES)})...")
    for url in DEFAULT_SCRAPE_SITES:
        articles = fetch_scrape(url)
        if not isinstance(articles, list):
            articles = []
        all_articles.extend(articles)

    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    return all_articles


# ØªØ³Øª Ù…Ø³ØªÙ‚ÛŒÙ…
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    news = fetch_all_news()
    for i, item in enumerate(news[:5], 1):
        print(f"{i}. {item['title']} ({item['source']})")
