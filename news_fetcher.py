"""
Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scraping
Ø¨Ø§ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ±Ù†Ø¯
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
import os
import json

from database import get_rss_sources, get_scrape_sources, is_sent, mark_sent, save_collected_news, get_collected_news

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DAILY_NEWS_DIR = "data/daily_news"
os.makedirs(DAILY_NEWS_DIR, exist_ok=True)


def save_daily_news(articles):
    """Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡"""
    if not articles:
        return
    today = datetime.now().strftime("%Y%m%d")
    file_path = os.path.join(DAILY_NEWS_DIR, f"daily_news_{today}.json")
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {file_path}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")


def fetch_rss_feed(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ÛŒÚ© ÙÛŒØ¯ RSS"""
    try:
        logger.info(f"ğŸ“° Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† RSS: {url[:50]}...")
        feed = feedparser.parse(url)
        articles = []

        for entry in feed.entries[:15]:
            link = entry.get("link", "")
            if not link or is_sent(link):
                continue

            published = entry.get("published_parsed") or entry.get("updated_parsed")
            pub_date = datetime(*published[:6]) if published else datetime.now()
            if (datetime.now() - pub_date).days > 7:
                continue

            title = entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
            summary = entry.get("summary", "") or entry.get("description", "")
            if summary:
                summary = BeautifulSoup(summary, "html.parser").get_text().strip()[:400]

            articles.append({
                "title": title,
                "link": link,
                "url": link,
                "summary": summary,
                "source": url,
                "published": pub_date.isoformat(),
            })
            mark_sent(link)

        logger.info(f"âœ… RSS: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles

    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url[:50]}: {e}")
        return []


def fetch_scraped_page(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ scraping Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² ØµÙØ­Ù‡"""
    try:
        logger.info(f"ğŸ•·ï¸ Ø¯Ø± Ø­Ø§Ù„ Scraping: {url[:50]}...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")
        articles = []
        links = soup.find_all("a", href=True)
        seen_in_this_page = set()

        for link in links[:30]:
            href = link.get("href", "")
            if href.startswith("/"):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            if not href.startswith("http") or href in seen_in_this_page or is_sent(href):
                continue

            keywords = ["news", "article", "cinema", "film", "movie", "entertainment", "/20"]
            if not any(keyword in href.lower() for keyword in keywords):
                continue

            title = link.get_text(strip=True)
            if len(title) < 15:
                continue
            title = " ".join(title.split())

            articles.append({
                "title": title,
                "link": href,
                "url": href,
                "summary": "",
                "source": url,
                "published": datetime.now().isoformat(),
            })

            seen_in_this_page.add(href)
            mark_sent(href)
            if len(articles) >= 10:
                break

        logger.info(f"âœ… Scrape: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles

    except requests.exceptions.Timeout:
        logger.error(f"â±ï¸ Timeout Ø¯Ø± Scraping {url[:50]}")
        return []
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []


def fetch_all_news():
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    logger.info("="*60)

    all_articles = []

    for rss_url in get_rss_sources():
        all_articles.extend(fetch_rss_feed(rss_url))

    for scrape_url in get_scrape_sources():
        all_articles.extend(fetch_scraped_page(scrape_url))

    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    
    if all_articles:
        save_collected_news(all_articles)  # Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        save_daily_news(all_articles)      # ÙØ§ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡

    return all_articles


if __name__ == "__main__":
    news = fetch_all_news()
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {len(news)}")
    if news:
        print(f"ğŸ“° Ø§ÙˆÙ„ÛŒÙ† Ø®Ø¨Ø±: {news[0]['title'][:60]}...")
