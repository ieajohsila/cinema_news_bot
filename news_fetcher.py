# news_fetcher.py

import logging
import feedparser
import httpx
from typing import List, Dict
from default_sources import DEFAULT_RSS_SOURCES, DEFAULT_SCRAPE_SITES

logger = logging.getLogger("news_fetcher")


def is_valid_rss_item(entry) -> bool:
    """
    ğŸ”§ FIX: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± RSS
    """
    title = entry.get("title", "").strip()
    link = entry.get("link", "").strip()
    
    # ÙÛŒÙ„ØªØ± Ø¹Ù†Ø§ÙˆÛŒÙ† Ø®Ø§Ù„ÛŒ
    if not title or not link:
        return False
    
    # ÙÛŒÙ„ØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ RSS Ø®ÙˆØ¯ Ù…Ù†Ø¨Ø¹
    if "/feed" in link.lower() or "/rss" in link.lower():
        return False
    
    # ÙÛŒÙ„ØªØ± Ø¹Ù†Ø§ÙˆÛŒÙ† Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†ÛŒ
    if len(title) < 10:
        return False
    
    # ÙÛŒÙ„ØªØ± Ø¹Ù†Ø§ÙˆÛŒÙ† Ú©Ù‡ ÙÙ‚Ø· Ù†Ø§Ù… Ø³Ø§ÛŒØª Ù‡Ø³ØªÙ†
    invalid_titles = [
        'latest news', 'Ø¢Ø®Ø±ÛŒÙ† Ø§Ø®Ø¨Ø§Ø±', 'home', 'feed',
        'rss', 'cinema', 'movies', 'news', 'homepage'
    ]
    if title.lower() in invalid_titles:
        return False
    
    return True


# =========================
# RSS FETCHER
# =========================
def fetch_rss_news() -> List[Dict]:
    news_list: List[Dict] = []

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS ({len(DEFAULT_RSS_SOURCES)})...")

    for url in DEFAULT_RSS_SOURCES:
        try:
            feed = feedparser.parse(url)
            count = 0

            for entry in feed.entries:
                # ğŸ”§ FIX: ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø±
                if not is_valid_rss_item(entry):
                    continue
                
                news = {
                    "title": entry.get("title", "").strip(),
                    "link": entry.get("link", "").strip(),
                    "summary": entry.get("summary", "").strip(),
                    "source": url,
                    "type": "rss",
                }

                news_list.append(news)
                count += 1

            logger.info(f"âœ… RSS: {count} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url}: {e}")

    return news_list


# =========================
# SCRAPER (SIMPLE & SAFE)
# =========================
def fetch_scrape_news() -> List[Dict]:
    news_list: List[Dict] = []

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Scrape ({len(DEFAULT_SCRAPE_SITES)})...")

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; CinemaNewsBot/1.0)"
    }

    for url in DEFAULT_SCRAPE_SITES:
        try:
            with httpx.Client(headers=headers, follow_redirects=True, timeout=15) as client:
                response = client.get(url)
                response.raise_for_status()

            # ğŸ”§ FIX: Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ù„ÛŒÙ†Ú© RSS Ù†Ø¨Ø§Ø´Ø¯
            if "/feed" in url.lower() or "/rss" in url.lower():
                logger.debug(f"âš ï¸ Scrape: Ù„ÛŒÙ†Ú© RSS Ø§Ø³ØªØŒ Ø±Ø¯ Ø´Ø¯: {url}")
                continue

            # ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ø±Ø§ Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (safe mode)
            news = {
                "title": f"Latest news from {url.split('/')[2]}",
                "link": url,
                "summary": "",
                "source": url,
                "type": "scrape",
            }

            news_list.append(news)
            logger.info(f"âœ… Scrape: 1 Ø¢ÛŒØªÙ… Ø§Ø² {url}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e}")

    return news_list


# =========================
# MAIN API (âš ï¸ Ø­ÛŒØ§ØªÛŒ)
# =========================
def fetch_all_news() -> List[Dict]:
    """
    âš ï¸ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù ÛŒØ§ rename Ø´ÙˆØ¯
    Admin Bot Ùˆ News Scheduler Ø¨Ù‡ Ø¢Ù† ÙˆØ§Ø¨Ø³ØªÙ‡â€ŒØ§Ù†Ø¯
    """

    all_news: List[Dict] = []

    rss_news = fetch_rss_news()
    scrape_news = fetch_scrape_news()

    all_news.extend(rss_news)
    all_news.extend(scrape_news)

    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_news)} Ø®Ø¨Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")

    return all_news
