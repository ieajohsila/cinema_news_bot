import logging
from bs4 import BeautifulSoup
import httpx
import feedparser
from default_sources import DEFAULT_RSS_SOURCES, DEFAULT_SCRAPE_SITES

logger = logging.getLogger("news_fetcher")
logging.basicConfig(level=logging.INFO)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
}

def fetch_rss_news(url):
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² RSS"""
    try:
        feed = feedparser.parse(url)
        news_items = []
        for entry in feed.entries:
            link = entry.get('link')
            title = entry.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')
            summary = entry.get('summary', '')
            if link:
                news_items.append({"title": title, "link": link, "summary": summary})
            else:
                logger.warning(f"Ø®Ø¨Ø± Ù†Ø§Ù‚Øµ Ø¯Ø± RSS: {title}")
        logger.info(f"âœ… RSS: {len(news_items)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")
        return news_items
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url}: {e}")
        return []

def scrape_news(url):
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ Ø¨Ø§ Scraping"""
    try:
        with httpx.Client(headers=HEADERS, follow_redirects=True, timeout=15) as client:
            response = client.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            news_items = []

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ùˆ Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ Ø§Ø² ØªÚ¯ <a> Ø¨Ø§ href Ú©Ø§Ù…Ù„
            for a_tag in soup.find_all('a', href=True):
                link = a_tag['href']
                if not link.startswith("http"):
                    continue  # Ù„ÛŒÙ†Ú© Ù†Ø§Ù‚Øµ Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯
                title = a_tag.get_text(strip=True) or "Ø®Ø¨Ø± Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
                news_items.append({"title": title, "link": link})

            logger.info(f"âœ… Scrape: {len(news_items)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")
            return news_items

    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e.response.status_code} {e.response.reason_phrase}")
        return []
    except httpx.RequestError as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e}")
        return []

def collect_all_news():
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø§Ø² RSS Ùˆ Scrape"""
    all_news = []

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS ({len(DEFAULT_RSS_SOURCES)})...")
    for rss in DEFAULT_RSS_SOURCES:
        all_news.extend(fetch_rss_news(rss))

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Scrape ({len(DEFAULT_SCRAPE_SITES)})...")
    for site in DEFAULT_SCRAPE_SITES:
        all_news.extend(scrape_news(site))

    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_news)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    return all_news

if __name__ == "__main__":
    news = collect_all_news()
    # Ø¨Ø±Ø§ÛŒ ØªØ³ØªØŒ 5 Ø®Ø¨Ø± Ø§ÙˆÙ„ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    for n in news[:5]:
        print(n["title"], n["link"])
