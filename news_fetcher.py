"""
Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scraping
Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import logging
from database import get_rss_sources, get_scrape_sources, is_sent, mark_sent, save_collected_news

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def fetch_rss_feed(url):
    try:
        logger.info(f"ğŸ“° Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† RSS: {url[:50]}...")
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:15]:
            link = entry.get("link", "")
            if not link or is_sent(link):
                continue
            published = entry.get("published_parsed") or entry.get("updated_parsed")
            if published:
                try:
                    pub_date = datetime(*published[:6])
                except:
                    pub_date = datetime.now()
            else:
                pub_date = datetime.now()
            if (datetime.now() - pub_date).days > 7:
                continue
            title = entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
            summary = entry.get("summary", "") or entry.get("description", "")
            if summary:
                soup = BeautifulSoup(summary, "html.parser")
                summary = soup.get_text().strip()[:400]
            articles.append({
                "title": title,
                "link": link,
                "url": link,
                "summary": summary,
                "source": url,
                "published": pub_date.isoformat(),
            })
            mark_sent(link)
        logger.info(f"âœ… RSS: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url[:50]}: {e}")
        return []


def fetch_scraped_page(url):
    try:
        logger.info(f"ğŸ•·ï¸  Ø¯Ø± Ø­Ø§Ù„ Scraping: {url[:50]}...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        articles = []
        links = soup.find_all("a", href=True)
        seen_in_page = set()
        for link in links[:30]:
            href = link.get("href", "")
            if href.startswith("/"):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            if not href.startswith("http") or href in seen_in_page or is_sent(href):
                continue
            keywords = ["news", "article", "cinema", "film", "movie", "entertainment", "/20"]
            if not any(k in href.lower() for k in keywords):
                continue
            title = link.get_text(strip=True)
            if len(title) < 15:
                continue
            title = " ".join(title.split())
            articles.append({
                "title": title,
                "link": href,
                "url": href,
                "summary": "",
                "source": url,
                "published": datetime.now().isoformat(),
            })
            seen_in_page.add(href)
            mark_sent(href)
            if len(articles) >= 10:
                break
        logger.info(f"âœ… Scrape: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []


def fetch_all_news():
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    all_articles = []
    rss_sources = get_rss_sources()
    logger.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ RSS: {len(rss_sources)}")
    for rss in rss_sources:
        all_articles.extend(fetch_rss_feed(rss))
    scrape_sources = get_scrape_sources()
    logger.info(f"ğŸ•·ï¸  ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ Scraping: {len(scrape_sources)}")
    for scrape in scrape_sources:
        all_articles.extend(fetch_scraped_page(scrape))
    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    if all_articles:
        save_collected_news(all_articles)
        logger.info("ğŸ’¾ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯")
    return all_articles


if __name__ == "__main__":
    news = fetch_all_news()
    print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {len(news)}")
    if news:
        print(f"ğŸ“° Ø§ÙˆÙ„ÛŒÙ† Ø®Ø¨Ø±: {news[0]['title'][:60]}...")
