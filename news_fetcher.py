# news_fetcher.py

import logging
import feedparser
import httpx
from typing import List, Dict
from default_sources import DEFAULT_RSS_SOURCES, DEFAULT_SCRAPE_SITES

logger = logging.getLogger("news_fetcher")


# =========================
# RSS FETCHER
# =========================
def fetch_rss_news() -> List[Dict]:
    news_list: List[Dict] = []

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS ({len(DEFAULT_RSS_SOURCES)})...")

    for url in DEFAULT_RSS_SOURCES:
        try:
            feed = feedparser.parse(url)
            count = 0

            for entry in feed.entries:
                news = {
                    "title": entry.get("title", "").strip(),
                    "link": entry.get("link", "").strip(),
                    "summary": entry.get("summary", "").strip(),
                    "source": url,
                    "type": "rss",
                }

                # Ø­Ø¯Ø§Ù‚Ù„ Ø¯ÛŒØªØ§
                if news["title"] and news["link"]:
                    news_list.append(news)
                    count += 1

            logger.info(f"âœ… RSS: {count} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url}: {e}")

    return news_list


# =========================
# SCRAPER (SIMPLE & SAFE)
# =========================
def fetch_scrape_news() -> List[Dict]:
    news_list: List[Dict] = []

    logger.info(f"ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Scrape ({len(DEFAULT_SCRAPE_SITES)})...")

    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; CinemaNewsBot/1.0)"
    }

    for url in DEFAULT_SCRAPE_SITES:
        try:
            with httpx.Client(headers=headers, follow_redirects=True, timeout=15) as client:
                response = client.get(url)
                response.raise_for_status()

            # ÙØ¹Ù„Ø§Ù‹ ÙÙ‚Ø· Ù„ÛŒÙ†Ú© ØµÙØ­Ù‡ Ø±Ø§ Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (safe mode)
            news = {
                "title": f"Latest news from {url}",
                "link": url,
                "summary": "",
                "source": url,
                "type": "scrape",
            }

            news_list.append(news)
            logger.info(f"âœ… Scrape: 1 Ø¢ÛŒØªÙ… Ø§Ø² {url}")

        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url}: {e}")

    return news_list


# =========================
# MAIN API (âš ï¸ Ø­ÛŒØ§ØªÛŒ)
# =========================
def fetch_all_news() -> List[Dict]:
    """
    âš ï¸ Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù†Ø¨Ø§ÛŒØ¯ Ø­Ø°Ù ÛŒØ§ rename Ø´ÙˆØ¯
    Admin Bot Ùˆ News Scheduler Ø¨Ù‡ Ø¢Ù† ÙˆØ§Ø¨Ø³ØªÙ‡â€ŒØ§Ù†Ø¯
    """

    all_news: List[Dict] = []

    rss_news = fetch_rss_news()
    scrape_news = fetch_scrape_news()

    all_news.extend(rss_news)
    all_news.extend(scrape_news)

    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_news)} Ø®Ø¨Ø± Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")

    return all_news
