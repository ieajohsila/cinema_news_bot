"""
Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scraping
Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging
import os
import json

from database import get_rss_sources, get_scrape_sources, is_sent, mark_sent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DAILY_NEWS_DIR = "data/daily_news"
os.makedirs(DAILY_NEWS_DIR, exist_ok=True)


def save_collected_news(all_articles):
    """Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± ÙØ§ÛŒÙ„ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    today_str = datetime.now().strftime("%Y-%m-%d")
    file_path = os.path.join(DAILY_NEWS_DIR, f"{today_str}.json")
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(all_articles, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ {len(all_articles)} Ø®Ø¨Ø± Ø¯Ø± {file_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")


def get_collected_news(date_str=None):
    """Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡"""
    if not date_str:
        date_str = datetime.now().strftime("%Y-%m-%d")
    file_path = os.path.join(DAILY_NEWS_DIR, f"{date_str}.json")
    
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ø®Ø¨Ø§Ø± Ø±ÙˆØ²Ø§Ù†Ù‡: {e}")
    return []


def fetch_rss_feed(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ÛŒÚ© ÙÛŒØ¯ RSS"""
    try:
        logger.info(f"ğŸ“° Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† RSS: {url[:50]}...")
        feed = feedparser.parse(url)
        articles = []
        for entry in feed.entries[:15]:
            link = entry.get("link", "")
            if not link or is_sent(link):
                continue
            published = entry.get("published_parsed") or entry.get("updated_parsed")
            if published:
                try:
                    pub_date = datetime(*published[:6])
                except:
                    pub_date = datetime.now()
            else:
                pub_date = datetime.now()
            if (datetime.now() - pub_date).days > 7:
                continue
            title = entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
            summary = entry.get("summary", "") or entry.get("description", "")
            if summary:
                soup = BeautifulSoup(summary, "html.parser")
                summary = soup.get_text().strip()[:400]
            articles.append({
                "title": title,
                "link": link,
                "url": link,
                "summary": summary,
                "source": url,
                "published": pub_date.isoformat(),
            })
            mark_sent(link)
        logger.info(f"âœ… RSS: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url[:50]}: {e}")
        return []


def fetch_scraped_page(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ scraping Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² ØµÙØ­Ù‡"""
    try:
        logger.info(f"ğŸ•·ï¸  Ø¯Ø± Ø­Ø§Ù„ Scraping: {url[:50]}...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")
        articles = []
        links = soup.find_all("a", href=True)
        seen_in_this_page = set()
        for link in links[:30]:
            href = link.get("href", "")
            if href.startswith("/"):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            if not href.startswith("http"):
                continue
            if href in seen_in_this_page or is_sent(href):
                continue
            keywords = ["news", "article", "cinema", "film", "movie", "entertainment", "/20"]
            if not any(k in href.lower() for k in keywords):
                continue
            title = link.get_text(strip=True)
            if len(title) < 15:
                continue
            title = " ".join(title.split())
            articles.append({
                "title": title,
                "link": href,
                "url": href,
                "summary": "",
                "source": url,
                "published": datetime.now().isoformat(),
            })
            seen_in_this_page.add(href)
            mark_sent(href)
            if len(articles) >= 10:
                break
        logger.info(f"âœ… Scrape: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []


def fetch_all_news():
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    logger.info("="*60)
    all_articles = []
    rss_sources = get_rss_sources()
    for rss_url in rss_sources:
        all_articles.extend(fetch_rss_feed(rss_url))
    scrape_sources = get_scrape_sources()
    for scrape_url in scrape_sources:
        all_articles.extend(fetch_scraped_page(scrape_url))
    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    if all_articles:
        save_collected_news(all_articles)
    return all_articles


if __name__ == "__main__":
    news = fetch_all_news()
    print(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {len(news)}")
