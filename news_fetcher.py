"""
Ù…Ø§Ú˜ÙˆÙ„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ RSS Ùˆ Scraping
"""

import feedparser
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import logging

from database import get_rss_sources, get_scrape_sources, is_sent, mark_sent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def fetch_rss_feed(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ÛŒÚ© ÙÛŒØ¯ RSS"""
    try:
        logger.info(f"ğŸ“° Ø¯Ø± Ø­Ø§Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† RSS: {url[:50]}...")
        feed = feedparser.parse(url)
        articles = []
        
        for entry in feed.entries[:15]:  # ÙÙ‚Ø· 15 Ø®Ø¨Ø± Ø¢Ø®Ø±
            link = entry.get("link", "")
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ ÛŒØ§ Ù†Ù‡
            if not link or is_sent(link):
                continue
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§Ø±ÛŒØ®
            published = entry.get("published_parsed") or entry.get("updated_parsed")
            if published:
                try:
                    pub_date = datetime(*published[:6])
                except:
                    pub_date = datetime.now()
            else:
                pub_date = datetime.now()
            
            # ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± 7 Ø±ÙˆØ² Ø§Ø®ÛŒØ±
            if (datetime.now() - pub_date).days > 7:
                continue
            
            title = entry.get("title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†")
            summary = entry.get("summary", "") or entry.get("description", "")
            
            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ HTML Ø§Ø² summary
            if summary:
                soup = BeautifulSoup(summary, "html.parser")
                summary = soup.get_text().strip()[:400]
            
            articles.append({
                "title": title,
                "link": link,
                "summary": summary,
                "source": url,
                "published": pub_date.isoformat(),
            })
            
            # Ø¹Ù„Ø§Ù…Øªâ€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡
            mark_sent(link)
        
        logger.info(f"âœ… RSS: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± RSS {url[:50]}: {e}")
        return []


def fetch_scraped_page(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ scraping Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² ØµÙØ­Ù‡"""
    try:
        logger.info(f"ğŸ•·ï¸  Ø¯Ø± Ø­Ø§Ù„ Scraping: {url[:50]}...")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, "html.parser")
        articles = []
        
        # Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ: Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ
        links = soup.find_all("a", href=True)
        
        seen_in_this_page = set()
        
        for link in links[:30]:  # Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ 30 Ù„ÛŒÙ†Ú©
            href = link.get("href", "")
            
            # Ø§Ú¯Ø± Ù„ÛŒÙ†Ú© Ù†Ø³Ø¨ÛŒ Ø§Ø³ØªØŒ Ú©Ø§Ù…Ù„ Ú©Ù†ÛŒØ¯
            if href.startswith("/"):
                from urllib.parse import urljoin
                href = urljoin(url, href)
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† Ú©Ù‡ Ù„ÛŒÙ†Ú© Ù…Ø¹ØªØ¨Ø± Ø¨Ø§Ø´Ù‡
            if not href.startswith("http"):
                continue
            
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ù‡Ù…ÛŒÙ† ØµÙØ­Ù‡
            if href in seen_in_this_page:
                continue
            
            # Ú†Ú© Ú©Ø±Ø¯Ù† ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù† Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            if is_sent(href):
                continue
            
            # ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø®Ø¨Ø±
            keywords = ["news", "article", "cinema", "film", "movie", "entertainment", "/20"]
            if not any(keyword in href.lower() for keyword in keywords):
                continue
            
            title = link.get_text(strip=True)
            if len(title) < 15:  # Ø¹Ù†ÙˆØ§Ù† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡
                continue
            
            # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
            title = " ".join(title.split())
            
            articles.append({
                "title": title,
                "link": href,
                "summary": "",
                "source": url,
                "published": datetime.now().isoformat(),
            })
            
            seen_in_this_page.add(href)
            mark_sent(href)
            
            # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ø± ØµÙØ­Ù‡
            if len(articles) >= 10:
                break
        
        logger.info(f"âœ… Scrape: {len(articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø§Ø² {url[:30]}")
        return articles
        
    except requests.exceptions.Timeout:
        logger.error(f"â±ï¸  Timeout Ø¯Ø± Scraping {url[:50]}")
        return []
    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø´Ø¨Ú©Ù‡ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Scraping {url[:50]}: {e}")
        return []


def fetch_all_news():
    """Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ù‡Ù…Ù‡ Ù…Ù†Ø§Ø¨Ø¹"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØªÙ…Ø§Ù… Ù…Ù†Ø§Ø¨Ø¹...")
    logger.info("="*60)
    
    all_articles = []
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² RSS
    rss_sources = get_rss_sources()
    logger.info(f"ğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ RSS: {len(rss_sources)}")
    for rss_url in rss_sources:
        articles = fetch_rss_feed(rss_url)
        all_articles.extend(articles)
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø² Scraping
    scrape_sources = get_scrape_sources()
    logger.info(f"ğŸ•·ï¸  ØªØ¹Ø¯Ø§Ø¯ Ù…Ù†Ø§Ø¨Ø¹ Scraping: {len(scrape_sources)}")
    for scrape_url in scrape_sources:
        articles = fetch_scraped_page(scrape_url)
        all_articles.extend(articles)
    
    logger.info("="*60)
    logger.info(f"âœ… Ø¬Ù…Ø¹Ø§Ù‹ {len(all_articles)} Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯")
    logger.info("="*60 + "\n")
    
    return all_articles


if __name__ == "__main__":
    # ØªØ³Øª
    print("ğŸ§ª ØªØ³Øª Ù…Ø§Ú˜ÙˆÙ„ news_fetcher...\n")
    news = fetch_all_news()
    print(f"\nğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø±: {len(news)}")
    if news:
        print(f"ğŸ“° Ø§ÙˆÙ„ÛŒÙ† Ø®Ø¨Ø±: {news[0]['title'][:60]}...")
