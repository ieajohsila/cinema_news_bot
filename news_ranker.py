"""
Ù…Ø§Ú˜ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø§Ø®Ø¨Ø§Ø±
"""

import re
from collections import Counter
from datetime import datetime, timedelta
import logging

logger = logging.getLogger(__name__)


# Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ù‡Ù… Ø¯Ø± Ø¯Ù†ÛŒØ§ÛŒ Ø³ÛŒÙ†Ù…Ø§
IMPORTANT_KEYWORDS = {
    # Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ùˆ Ø¨Ø±Ù†Ø¯Ù‡Ø§
    "oscar", "academy", "cannes", "berlin", "venice", "sundance", "golden globe",
    "spielberg", "nolan", "tarantino", "scorsese", "coppola", "kubrick",
    "marvel", "disney", "warner", "netflix", "apple tv", "hbo", "amazon",
    
    # Ú˜Ø§Ù†Ø±Ù‡Ø§ Ùˆ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ù‡Ù…
    "box office", "blockbuster", "premiere", "release", "trailer", "teaser",
    "award", "nomination", "winner", "festival", "competition",
    "director", "actor", "actress", "cast", "star",
    "breaking", "exclusive", "announced", "confirmed",
    
    # Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ
    "Ø§Ø³Ú©Ø§Ø±", "Ø¬Ø§ÛŒØ²Ù‡", "ÙÛŒÙ„Ù…", "Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†", "Ø¨Ø§Ø²ÛŒÚ¯Ø±",
    "ÙØ±ÙˆØ´", "Ø§Ú©Ø±Ø§Ù†", "ØªØ±ÛŒÙ„Ø±", "Ø¬Ø´Ù†ÙˆØ§Ø±Ù‡", "Ú©Ù†",
}

# Ú©Ù„Ù…Ø§Øª Ù…Ù†ÙÛŒ (Ø§Ø®Ø¨Ø§Ø± Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª)
NEGATIVE_KEYWORDS = {
    "rumor", "speculation", "might", "could", "possibly", "allegedly",
    "unconfirmed", "gossip",
    "Ø´Ø§ÛŒØ¹Ù‡", "Ø§Ø­ØªÙ…Ø§Ù„", "Ù…Ù…Ú©Ù† Ø§Ø³Øª",
}

# Ú©Ù„Ù…Ø§Øª ÙÙˆØ±ÛŒ (Ø§Ù‡Ù…ÛŒØª Ø¨Ø§Ù„Ø§)
URGENT_KEYWORDS = {
    "breaking", "dies", "death", "dead", "passed away",
    "wins oscar", "oscar winner", "best picture",
    "record breaking", "historic", "unprecedented",
    "ÙÙˆØª", "Ø¯Ø±Ú¯Ø°Ø´Øª", "Ø¨Ø±Ù†Ø¯Ù‡ Ø§Ø³Ú©Ø§Ø±",
}


def calculate_importance(article):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª ÛŒÚ© Ø®Ø¨Ø± (0 ØªØ§ 3)"""
    score = 1.0  # Ø§Ù…ØªÛŒØ§Ø² Ù¾Ø§ÛŒÙ‡
    
    title = article.get("title", "").lower()
    summary = article.get("summary", "").lower()
    text = title + " " + summary
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª ÙÙˆØ±ÛŒ (Ø®ÛŒÙ„ÛŒ Ù…Ù‡Ù…)
    urgent_count = sum(1 for keyword in URGENT_KEYWORDS if keyword in text)
    if urgent_count > 0:
        score += 2
    
    # Ú†Ú© Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ù‡Ù…
    important_count = sum(1 for keyword in IMPORTANT_KEYWORDS if keyword in text)
    score += min(important_count * 0.5, 1.5)  # Ø­Ø¯Ø§Ú©Ø«Ø± +1.5 Ø§Ù…ØªÛŒØ§Ø²
    
    # Ú©Ù… Ú©Ø±Ø¯Ù† Ø§Ù…ØªÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ù†ÙÛŒ
    negative_count = sum(1 for keyword in NEGATIVE_KEYWORDS if keyword in text)
    score -= negative_count * 0.5
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§ (Ù…Ø­ØªÙˆØ§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ù‡Ù…â€ŒØªØ± Ø§Ø³Øª)
    if len(summary) > 250:
        score += 0.5
    elif len(summary) < 50:
        score -= 0.3
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªØ§Ø²Ú¯ÛŒ Ø®Ø¨Ø±
    published = article.get("published")
    if published:
        try:
            if isinstance(published, str):
                published = datetime.fromisoformat(published)
            
            age_hours = (datetime.now() - published).total_seconds() / 3600
            
            if age_hours < 6:  # Ø®ÛŒÙ„ÛŒ ØªØ§Ø²Ù‡
                score += 0.8
            elif age_hours < 24:  # ØªØ§Ø²Ù‡
                score += 0.5
            elif age_hours > 120:  # Ø®ÛŒÙ„ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (5 Ø±ÙˆØ²)
                score -= 0.8
        except:
            pass
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¹Ù†ÙˆØ§Ù† (Ø¹Ù†Ø§ÙˆÛŒÙ† Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ØªØ± Ù‡Ø³ØªÙ†Ø¯)
    if 30 < len(title) < 100:
        score += 0.2
    
    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ 0-3
    score = max(0, min(3, round(score)))
    
    return int(score)


def rank_news(articles, min_importance=1):
    """Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª"""
    if not articles:
        logger.info("ğŸ“­ Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯")
        return []
    
    logger.info(f"ğŸ“Š Ø´Ø±ÙˆØ¹ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ {len(articles)} Ø®Ø¨Ø±...")
    
    ranked = []
    importance_counts = {0: 0, 1: 0, 2: 0, 3: 0}
    
    for article in articles:
        importance = calculate_importance(article)
        importance_counts[importance] += 1
        
        if importance >= min_importance:
            article["importance"] = importance
            ranked.append(article)
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù‡Ù…ÛŒØª (Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§ÙˆÙ„)
    ranked.sort(key=lambda x: x["importance"], reverse=True)
    
    # Ù„Ø§Ú¯ Ø¢Ù…Ø§Ø±ÛŒ
    logger.info(f"ğŸ“ˆ Ø¢Ù…Ø§Ø± Ø§Ù‡Ù…ÛŒØª Ø§Ø®Ø¨Ø§Ø±:")
    logger.info(f"   â­â­â­ Ø³Ø·Ø­ 3 (ÙÙˆØ±ÛŒ): {importance_counts[3]} Ø®Ø¨Ø±")
    logger.info(f"   â­â­ Ø³Ø·Ø­ 2 (Ù…Ù‡Ù…): {importance_counts[2]} Ø®Ø¨Ø±")
    logger.info(f"   â­ Ø³Ø·Ø­ 1 (Ù…Ø¹Ù…ÙˆÙ„ÛŒ): {importance_counts[1]} Ø®Ø¨Ø±")
    logger.info(f"   â€¢ Ø³Ø·Ø­ 0 (Ú©Ù…â€ŒØ§Ù‡Ù…ÛŒØª): {importance_counts[0]} Ø®Ø¨Ø±")
    logger.info(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ù…Ù†ØªØ®Ø¨ (Ø­Ø¯Ø§Ù‚Ù„ Ø³Ø·Ø­ {min_importance}): {len(ranked)}")
    
    return ranked


def extract_keywords(text, min_length=4):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ú©Ù„Ù…Ø§Øª
    words = text.split()
    
    # ÙÛŒÙ„ØªØ± Ú©Ù„Ù…Ø§Øª (Ø­Ø¯Ø§Ù‚Ù„ Ø·ÙˆÙ„ Ùˆ Ø­Ø°Ù stop words Ø³Ø§Ø¯Ù‡)
    stop_words = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
        'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',
        'Ùˆ', 'ÛŒØ§', 'Ø¯Ø±', 'Ø¨Ù‡', 'Ø§Ø²', 'Ú©Ù‡', 'Ø§ÛŒÙ†', 'Ø¢Ù†', 'Ø±Ø§'
    }
    
    keywords = [
        word for word in words 
        if len(word) >= min_length and word not in stop_words
    ]
    
    return keywords


def find_common_topics(articles):
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø´ØªØ±Ú© Ø¨ÛŒÙ† Ø§Ø®Ø¨Ø§Ø±"""
    all_keywords = []
    
    for article in articles:
        title = article.get("title", "")
        summary = article.get("summary", "")
        text = title + " " + summary
        
        keywords = extract_keywords(text)
        all_keywords.extend(keywords)
    
    # Ø´Ù…Ø§Ø±Ø´ ÙØ±Ø§ÙˆØ§Ù†ÛŒ
    keyword_counts = Counter(all_keywords)
    
    # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† 10 Ú©Ù„Ù…Ù‡ Ù¾Ø±ØªÚ©Ø±Ø§Ø±
    return keyword_counts.most_common(10)


def generate_daily_trend(articles):
    """ØªÙˆÙ„ÛŒØ¯ Ø®Ù„Ø§ØµÙ‡ ØªØ±Ù†Ø¯ Ø±ÙˆØ²Ø§Ù†Ù‡"""
    if not articles:
        return "Ø§Ù…Ø±ÙˆØ² Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ù†Ø¨ÙˆØ¯."
    
    logger.info(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯ Ø§Ø² {len(articles)} Ø®Ø¨Ø±...")
    
    # ÙÙ‚Ø· Ø§Ø®Ø¨Ø§Ø± Ø§Ù…Ø±ÙˆØ²
    today = datetime.now().date()
    recent_articles = []
    
    for article in articles:
        pub_date = article.get("published")
        if pub_date:
            try:
                if isinstance(pub_date, str):
                    pub_date = datetime.fromisoformat(pub_date).date()
                elif isinstance(pub_date, datetime):
                    pub_date = pub_date.date()
                
                if (today - pub_date).days <= 1:  # Ø§Ù…Ø±ÙˆØ² Ùˆ Ø¯ÛŒØ±ÙˆØ²
                    recent_articles.append(article)
            except:
                pass
    
    if not recent_articles:
        return "Ø§Ù…Ø±ÙˆØ² Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ÛŒ Ù†Ø¨ÙˆØ¯."
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¯Ø§Øº
    topics = find_common_topics(recent_articles)
    
    if not topics:
        return f"Ø§Ù…Ø±ÙˆØ² {len(recent_articles)} Ø®Ø¨Ø± Ù…Ù†ØªØ´Ø± Ø´Ø¯."
    
    # Ø³Ø§Ø®Øª Ø®Ù„Ø§ØµÙ‡
    summary = f"ğŸ“Š *ØªØ±Ù†Ø¯ Ø§Ù…Ø±ÙˆØ² Ø³ÛŒÙ†Ù…Ø§*\n\n"
    summary += f"ğŸ”¥ *Ø¯Ø§Øºâ€ŒØªØ±ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª:*\n"
    
    for i, (topic, count) in enumerate(topics[:5], 1):
        summary += f"{i}. {topic.title()} ({count} Ø¨Ø§Ø±)\n"
    
    summary += f"\nğŸ“° ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø§Ø®Ø¨Ø§Ø±: {len(recent_articles)}"
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø®Ø¨Ø±
    important_news = [a for a in recent_articles if calculate_importance(a) >= 2]
    if important_news:
        summary += f"\nâ­ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ù…Ù‡Ù…: {len(important_news)}"
        
        # Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ø®Ø¨Ø±
        top_news = max(important_news, key=lambda x: calculate_importance(x))
        summary += f"\n\nğŸŒŸ *Ø¨Ø±Ø¬Ø³ØªÙ‡â€ŒØªØ±ÛŒÙ† Ø®Ø¨Ø±:*\n{top_news['title'][:100]}"
    
    return summary


if __name__ == "__main__":
    # ØªØ³Øª
    test_articles = [
        {
            "title": "Breaking: Director Christopher Nolan wins Oscar for Best Picture",
            "summary": "Historic achievement in cinema",
            "published": datetime.now().isoformat(),
        },
        {
            "title": "New Marvel movie trailer released",
            "summary": "Fans excited for upcoming blockbuster",
            "published": (datetime.now() - timedelta(hours=5)).isoformat(),
        }
    ]
    
    print("ğŸ§ª ØªØ³Øª Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ...\n")
    ranked = rank_news(test_articles, min_importance=1)
    
    for news in ranked:
        print(f"\nâ­ Ø§Ù‡Ù…ÛŒØª: {news['importance']}")
        print(f"ğŸ“° {news['title']}")
    
    print("\n\nğŸ“Š ØªØ³Øª ØªØ±Ù†Ø¯...\n")
    trend = generate_daily_trend(test_articles)
    print(trend)
